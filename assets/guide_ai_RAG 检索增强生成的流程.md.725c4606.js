import{_ as a,o as e,c as t,Q as r}from"./chunks/framework.b6910bb2.js";const i="/vitePress-blob/assets/5.ca3137a4.png",m=JSON.parse('{"title":"RAG 检索增强生成的流程","description":"","frontmatter":{},"headers":[],"relativePath":"guide/ai/RAG 检索增强生成的流程.md","filePath":"guide/ai/RAG 检索增强生成的流程.md","lastUpdated":1736921952000}'),o={name:"guide/ai/RAG 检索增强生成的流程.md"},l=r('<h1 id="rag-检索增强生成的流程" tabindex="-1">RAG 检索增强生成的流程 <a class="header-anchor" href="#rag-检索增强生成的流程" aria-label="Permalink to &quot;RAG 检索增强生成的流程&quot;">​</a></h1><p>RAG 其全称是 Retrieval Augmented Generation，可以被翻译成 检索增强生成技术，从标题上也能了解其核心的流程 检索 =&gt; 增强 =&gt; 生成。</p><h2 id="llm-的局限" tabindex="-1">LLM 的局限 <a class="header-anchor" href="#llm-的局限" aria-label="Permalink to &quot;LLM 的局限&quot;">​</a></h2><p>在介绍 RAG 之前，我们先来了解一下 LLM 的局限性。</p><p>首先是幻觉问题（hallucination），LLM 底层还不具备真正的逻辑推理能力，是根据大量的数据进行概率性预测，所以在某些情况下，LLM 会生成一些不合理的答案。</p><p>其次是对领域知识的欠缺，造成这个问题主要是俩个原因，第一个是对知识的更新慢，另一个是对专业领域的知识训练样本不足导致。</p><h2 id="rag-的优势" tabindex="-1">RAG 的优势 <a class="header-anchor" href="#rag-的优势" aria-label="Permalink to &quot;RAG 的优势&quot;">​</a></h2><p>当我们了解了 LLM 的局限性后，RAG 会尽可能提供与答案相关的上下文，来增强它正确输出的可能性。</p><p>RAG 的优势主要体现在以下几个方面：</p><ol><li>用户输入提问</li><li>检索：根据用户提问对 向量数据库 进行相似性检测，查找与回答用户问题最相关的内容</li><li>增强：根据检索的结果，生成 prompt。 一般都会涉及 “仅依赖下述信息源来回答问题” 这种限制 LLM 参考信息源的语句，来减少幻想，让回答更加聚焦</li><li>生成：将增强后的 prompt 传递给 LLM，返回数据给用户</li></ol><p>所以 RAG 就是哪里有问题解决哪里，既然大模型无法获得最新和内部的数据集，那我们就使用外挂的向量数据库为 LLM 提供最新和内部的数据库。既然大模型有幻想问题，我们就将回答问题所需要的信息和知识编码到上下文中，强制大模型只参考这些内容进行回答。</p><p>注意：LLM 是逻辑推理引擎，而不是信息引擎。所以，由外挂的向量数据库提供最有效的知识，然后由 LLM 根据知识进行推理，提供有价值的回复。</p><h2 id="rag-流程" tabindex="-1">RAG 流程 <a class="header-anchor" href="#rag-流程" aria-label="Permalink to &quot;RAG 流程&quot;">​</a></h2><p><img src="'+i+'" alt="输出结果"></p><p>后续章节，在下将针对 RAG 的各个流程做详细的介绍。</p>',15),_=[l];function p(s,n,c,d,h,L){return e(),t("div",null,_)}const R=a(o,[["render",p]]);export{m as __pageData,R as default};
