import{_ as s,o as a,c as n,Q as l}from"./chunks/framework.b6910bb2.js";const o="/vitePress-blob/assets/1.111604b1.png",p="/vitePress-blob/assets/2.3cc0a64c.png",e="/vitePress-blob/assets/3.6ba488f9.png",F=JSON.parse('{"title":"LangChain 快速入门","description":"","frontmatter":{},"headers":[],"relativePath":"guide/ai/langchain快速入门.md","filePath":"guide/ai/langchain快速入门.md","lastUpdated":1718099618000}'),t={name:"guide/ai/langchain快速入门.md"},c=l(`<h1 id="langchain-快速入门" tabindex="-1">LangChain 快速入门 <a class="header-anchor" href="#langchain-快速入门" aria-label="Permalink to &quot;LangChain 快速入门&quot;">​</a></h1><h2 id="安装" tabindex="-1">安装 <a class="header-anchor" href="#安装" aria-label="Permalink to &quot;安装&quot;">​</a></h2><p>要安装这个包，你可以使用 npm 或 yarn：</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#B392F0;">yarn</span><span style="color:#E1E4E8;"> </span><span style="color:#9ECBFF;">add</span><span style="color:#E1E4E8;"> </span><span style="color:#9ECBFF;">langchain</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#6F42C1;">yarn</span><span style="color:#24292E;"> </span><span style="color:#032F62;">add</span><span style="color:#24292E;"> </span><span style="color:#032F62;">langchain</span></span></code></pre></div><h2 id="安装环境" tabindex="-1">安装环境 <a class="header-anchor" href="#安装环境" aria-label="Permalink to &quot;安装环境&quot;">​</a></h2><p>node &gt;= 18.x</p><h2 id="什么是-lcel-langchain-expression-language" tabindex="-1">什么是 LCEL (LangChain Expression Language) <a class="header-anchor" href="#什么是-lcel-langchain-expression-language" aria-label="Permalink to &quot;什么是 LCEL (LangChain Expression Language)&quot;">​</a></h2><p>LangChain Expression Language is a way to create arbitrary custom chains. It is built on the <a href="https://v02.api.js.langchain.com/classes/langchain_core_runnables.Runnable.html" target="_blank" rel="noreferrer">Runnable</a> protocol.</p><p>LCEL 无论是 python 还是 js 版本都在主推的新设计，能创建自定义的链，它是基于 <a href="https://v02.api.js.langchain.com/classes/langchain_core_runnables.Runnable.html" target="_blank" rel="noreferrer">Runnable</a> 协议构建的。</p><h2 id="通过-langchain-加载大模型" tabindex="-1">通过 LangChain 加载大模型 <a class="header-anchor" href="#通过-langchain-加载大模型" aria-label="Permalink to &quot;通过 LangChain 加载大模型&quot;">​</a></h2><h3 id="本地大模型" tabindex="-1">本地大模型 <a class="header-anchor" href="#本地大模型" aria-label="Permalink to &quot;本地大模型&quot;">​</a></h3><p>在 mac 平台下，推荐用 <a href="https://ollama.com/" target="_blank" rel="noreferrer">ollama</a>，使用简单，下载好模型后，点击这个 app，就会自动在 <a href="http://localhost:11434" target="_blank" rel="noreferrer">http://localhost:11434</a> 起一个 llm 的服务。</p><h4 id="安装-llama3-模型" tabindex="-1">安装 llama3 模型 <a class="header-anchor" href="#安装-llama3-模型" aria-label="Permalink to &quot;安装 llama3 模型&quot;">​</a></h4><p>可以通过 <code>ollama list</code> 查看所有的模型，然后通过 <code>ollama pull</code> 安装模型。</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#6A737D;"># 模型列表参考：https://ollama.com/library</span></span>
<span class="line"><span style="color:#B392F0;">ollama</span><span style="color:#E1E4E8;"> </span><span style="color:#9ECBFF;">pull</span><span style="color:#E1E4E8;"> [model]</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#6A737D;"># 模型列表参考：https://ollama.com/library</span></span>
<span class="line"><span style="color:#6F42C1;">ollama</span><span style="color:#24292E;"> </span><span style="color:#032F62;">pull</span><span style="color:#24292E;"> [model]</span></span></code></pre></div><p>目前下载了一个 ollama3 的模型</p><p><img src="`+o+`" alt="初始状态图"></p><p>当我们在本地启动了一个模型后，我们就可以通过 LangChain 来调用这个模型了。</p><div class="language-js vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">js</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> { Ollama } </span><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> </span><span style="color:#9ECBFF;">&#39;@langchain/community/llms/ollama&#39;</span><span style="color:#E1E4E8;">;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">const</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">ollama</span><span style="color:#E1E4E8;"> </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#F97583;">new</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">Ollama</span><span style="color:#E1E4E8;">({</span></span>
<span class="line"><span style="color:#E1E4E8;">  baseUrl: </span><span style="color:#9ECBFF;">&quot;http://localhost:11434&quot;</span><span style="color:#E1E4E8;">, </span></span>
<span class="line"><span style="color:#E1E4E8;">  model: </span><span style="color:#9ECBFF;">&quot;llama3&quot;</span><span style="color:#E1E4E8;">, </span></span>
<span class="line"><span style="color:#E1E4E8;">});</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">ollama.</span><span style="color:#B392F0;">invoke</span><span style="color:#E1E4E8;">(</span><span style="color:#9ECBFF;">&quot;用中文讲一个笑话&quot;</span><span style="color:#E1E4E8;">).</span><span style="color:#B392F0;">then</span><span style="color:#E1E4E8;">((</span><span style="color:#FFAB70;">res</span><span style="color:#E1E4E8;">) </span><span style="color:#F97583;">=&gt;</span><span style="color:#E1E4E8;"> {</span></span>
<span class="line"><span style="color:#E1E4E8;">  console.</span><span style="color:#B392F0;">log</span><span style="color:#E1E4E8;">(</span><span style="color:#9ECBFF;">&#39;res&#39;</span><span style="color:#E1E4E8;">, res);</span></span>
<span class="line"><span style="color:#E1E4E8;">});</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#D73A49;">import</span><span style="color:#24292E;"> { Ollama } </span><span style="color:#D73A49;">from</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&#39;@langchain/community/llms/ollama&#39;</span><span style="color:#24292E;">;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#D73A49;">const</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">ollama</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">new</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">Ollama</span><span style="color:#24292E;">({</span></span>
<span class="line"><span style="color:#24292E;">  baseUrl: </span><span style="color:#032F62;">&quot;http://localhost:11434&quot;</span><span style="color:#24292E;">, </span></span>
<span class="line"><span style="color:#24292E;">  model: </span><span style="color:#032F62;">&quot;llama3&quot;</span><span style="color:#24292E;">, </span></span>
<span class="line"><span style="color:#24292E;">});</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">ollama.</span><span style="color:#6F42C1;">invoke</span><span style="color:#24292E;">(</span><span style="color:#032F62;">&quot;用中文讲一个笑话&quot;</span><span style="color:#24292E;">).</span><span style="color:#6F42C1;">then</span><span style="color:#24292E;">((</span><span style="color:#E36209;">res</span><span style="color:#24292E;">) </span><span style="color:#D73A49;">=&gt;</span><span style="color:#24292E;"> {</span></span>
<span class="line"><span style="color:#24292E;">  console.</span><span style="color:#6F42C1;">log</span><span style="color:#24292E;">(</span><span style="color:#032F62;">&#39;res&#39;</span><span style="color:#24292E;">, res);</span></span>
<span class="line"><span style="color:#24292E;">});</span></span></code></pre></div><p>输出结果 👇</p><p><img src="`+p+`" alt="输出结果"></p><h3 id="云端大模型" tabindex="-1">云端大模型 <a class="header-anchor" href="#云端大模型" aria-label="Permalink to &quot;云端大模型&quot;">​</a></h3><h4 id="openai" tabindex="-1">OpenAI <a class="header-anchor" href="#openai" aria-label="Permalink to &quot;OpenAI&quot;">​</a></h4><p>在调用 OpenAI 之前需要先申请一个 API Key，然后将 API Key 设置为环境变量 <code>OPENAI_API_KEY</code>。</p><p>然后通过 langchain 的 OpenAI 类来调用 OpenAI 的模型。</p><p>调用之前需要先 安装 <code>@langchain/openai</code> 包：</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#B392F0;">//</span><span style="color:#E1E4E8;"> </span><span style="color:#9ECBFF;">安装</span><span style="color:#E1E4E8;"> </span><span style="color:#9ECBFF;">openai</span><span style="color:#E1E4E8;"> </span><span style="color:#9ECBFF;">包</span></span>
<span class="line"><span style="color:#B392F0;">yarn</span><span style="color:#E1E4E8;"> </span><span style="color:#9ECBFF;">add</span><span style="color:#E1E4E8;"> </span><span style="color:#9ECBFF;">@langchain/openai</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#6F42C1;">//</span><span style="color:#24292E;"> </span><span style="color:#032F62;">安装</span><span style="color:#24292E;"> </span><span style="color:#032F62;">openai</span><span style="color:#24292E;"> </span><span style="color:#032F62;">包</span></span>
<span class="line"><span style="color:#6F42C1;">yarn</span><span style="color:#24292E;"> </span><span style="color:#032F62;">add</span><span style="color:#24292E;"> </span><span style="color:#032F62;">@langchain/openai</span></span></code></pre></div><div class="language-js vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">js</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> { OpenAI } </span><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> </span><span style="color:#9ECBFF;">&quot;@langchain/openai&quot;</span><span style="color:#E1E4E8;">;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">const</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">model</span><span style="color:#E1E4E8;"> </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#F97583;">new</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">OpenAI</span><span style="color:#E1E4E8;">({</span></span>
<span class="line"><span style="color:#E1E4E8;">  model: </span><span style="color:#9ECBFF;">&quot;gpt-3.5-turbo-instruct&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#6A737D;">// Defaults to &quot;gpt-3.5-turbo-instruct&quot; if no model provided.</span></span>
<span class="line"><span style="color:#E1E4E8;">  temperature: </span><span style="color:#79B8FF;">0.9</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">  apiKey: </span><span style="color:#9ECBFF;">&quot;YOUR-API-KEY&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#6A737D;">// In Node.js defaults to p<wbr>rocess.env.OPENAI_API_KEY</span></span>
<span class="line"><span style="color:#E1E4E8;">});</span></span>
<span class="line"><span style="color:#F97583;">const</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">res</span><span style="color:#E1E4E8;"> </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#F97583;">await</span><span style="color:#E1E4E8;"> model.</span><span style="color:#B392F0;">invoke</span><span style="color:#E1E4E8;">(</span></span>
<span class="line"><span style="color:#E1E4E8;">  </span><span style="color:#9ECBFF;">&quot;What would be a good company name a company that makes colorful socks?&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">);</span></span>
<span class="line"><span style="color:#E1E4E8;">console.</span><span style="color:#B392F0;">log</span><span style="color:#E1E4E8;">({ res });</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#D73A49;">import</span><span style="color:#24292E;"> { OpenAI } </span><span style="color:#D73A49;">from</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&quot;@langchain/openai&quot;</span><span style="color:#24292E;">;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#D73A49;">const</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">model</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">new</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">OpenAI</span><span style="color:#24292E;">({</span></span>
<span class="line"><span style="color:#24292E;">  model: </span><span style="color:#032F62;">&quot;gpt-3.5-turbo-instruct&quot;</span><span style="color:#24292E;">, </span><span style="color:#6A737D;">// Defaults to &quot;gpt-3.5-turbo-instruct&quot; if no model provided.</span></span>
<span class="line"><span style="color:#24292E;">  temperature: </span><span style="color:#005CC5;">0.9</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">  apiKey: </span><span style="color:#032F62;">&quot;YOUR-API-KEY&quot;</span><span style="color:#24292E;">, </span><span style="color:#6A737D;">// In Node.js defaults to p<wbr>rocess.env.OPENAI_API_KEY</span></span>
<span class="line"><span style="color:#24292E;">});</span></span>
<span class="line"><span style="color:#D73A49;">const</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">res</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">await</span><span style="color:#24292E;"> model.</span><span style="color:#6F42C1;">invoke</span><span style="color:#24292E;">(</span></span>
<span class="line"><span style="color:#24292E;">  </span><span style="color:#032F62;">&quot;What would be a good company name a company that makes colorful socks?&quot;</span></span>
<span class="line"><span style="color:#24292E;">);</span></span>
<span class="line"><span style="color:#24292E;">console.</span><span style="color:#6F42C1;">log</span><span style="color:#24292E;">({ res });</span></span></code></pre></div><h4 id="百度大模型" tabindex="-1">百度大模型 <a class="header-anchor" href="#百度大模型" aria-label="Permalink to &quot;百度大模型&quot;">​</a></h4><p>因为 OpenAI 在国内比较难访问，所以我们可以使用第三方大模型来避规访问慢的问题，这里以百度的大模型为例。</p><p>使用该嵌入模型需要 API 密钥。您可以通过 <a href="https://console.bce.baidu.com/qianfan/ais/console/applicationConsole/application" target="_blank" rel="noreferrer">https://console.bce.baidu.com/qianfan/ais/console/applicationConsole/application</a> 创建应用注册获取 API_KEY 和 SECRET_KEY。</p><p>请将获取的API密钥设置为名为 BAIDU_API_KEY 的环境变量，并将您的密钥设置为名为BAIDU_SECRET_KEY 的环境变量。</p><div class="language-js vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">js</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#6A737D;">// 您需要安装该 @langchain/community 包：</span></span>
<span class="line"><span style="color:#E1E4E8;">yarn add @langchain</span><span style="color:#F97583;">/</span><span style="color:#E1E4E8;">community</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#6A737D;">// 您需要安装该 @langchain/community 包：</span></span>
<span class="line"><span style="color:#24292E;">yarn add @langchain</span><span style="color:#D73A49;">/</span><span style="color:#24292E;">community</span></span></code></pre></div><div class="language-js vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">js</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> { ChatBaiduWenxin } </span><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> </span><span style="color:#9ECBFF;">&quot;@langchain/community/chat_models/baiduwenxin&quot;</span><span style="color:#E1E4E8;">;</span></span>
<span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> { HumanMessage } </span><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> </span><span style="color:#9ECBFF;">&quot;@langchain/core/messages&quot;</span><span style="color:#E1E4E8;">;</span></span>
<span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> </span><span style="color:#9ECBFF;">&#39;dotenv/config&#39;</span><span style="color:#E1E4E8;">;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">const</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">ernieTurbo</span><span style="color:#E1E4E8;"> </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#F97583;">new</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">ChatBaiduWenxin</span><span style="color:#E1E4E8;">({</span></span>
<span class="line"><span style="color:#E1E4E8;">    modelName: p<wbr>rocess.env.</span><span style="color:#79B8FF;">BAIDU_MODEL_NAME</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">});</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">const</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">messages</span><span style="color:#E1E4E8;"> </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span><span style="color:#F97583;">new</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">HumanMessage</span><span style="color:#E1E4E8;">(</span><span style="color:#9ECBFF;">&quot;讲一个笑话&quot;</span><span style="color:#E1E4E8;">)];</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">const</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">res</span><span style="color:#E1E4E8;"> </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#F97583;">await</span><span style="color:#E1E4E8;"> ernieTurbo.</span><span style="color:#B392F0;">invoke</span><span style="color:#E1E4E8;">(messages);</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">console.</span><span style="color:#B392F0;">log</span><span style="color:#E1E4E8;">(res);</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#D73A49;">import</span><span style="color:#24292E;"> { ChatBaiduWenxin } </span><span style="color:#D73A49;">from</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&quot;@langchain/community/chat_models/baiduwenxin&quot;</span><span style="color:#24292E;">;</span></span>
<span class="line"><span style="color:#D73A49;">import</span><span style="color:#24292E;"> { HumanMessage } </span><span style="color:#D73A49;">from</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&quot;@langchain/core/messages&quot;</span><span style="color:#24292E;">;</span></span>
<span class="line"><span style="color:#D73A49;">import</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&#39;dotenv/config&#39;</span><span style="color:#24292E;">;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#D73A49;">const</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">ernieTurbo</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">new</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">ChatBaiduWenxin</span><span style="color:#24292E;">({</span></span>
<span class="line"><span style="color:#24292E;">    modelName: p<wbr>rocess.env.</span><span style="color:#005CC5;">BAIDU_MODEL_NAME</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">});</span></span>
<span class="line"></span>
<span class="line"><span style="color:#D73A49;">const</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">messages</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> [</span><span style="color:#D73A49;">new</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">HumanMessage</span><span style="color:#24292E;">(</span><span style="color:#032F62;">&quot;讲一个笑话&quot;</span><span style="color:#24292E;">)];</span></span>
<span class="line"></span>
<span class="line"><span style="color:#D73A49;">const</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">res</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">await</span><span style="color:#24292E;"> ernieTurbo.</span><span style="color:#6F42C1;">invoke</span><span style="color:#24292E;">(messages);</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">console.</span><span style="color:#6F42C1;">log</span><span style="color:#24292E;">(res);</span></span></code></pre></div><p><img src="`+e+'" alt="输出结果"></p><p>⚠️注意: 如果出现 API 调用次数有所限制，需要在 <a href="https://console.bce.baidu.com/qianfan/ais/console/onlineService" target="_blank" rel="noreferrer">https://console.bce.baidu.com/qianfan/ais/console/onlineService</a> 开通对应的模型服务。</p><h2 id="lcel-有什么优势" tabindex="-1">LCEL 有什么优势 <a class="header-anchor" href="#lcel-有什么优势" aria-label="Permalink to &quot;LCEL 有什么优势&quot;">​</a></h2><p>LCEL 从底层设计的目标就是支持 <strong>从原型到生产</strong> 完整流程不需要修改任何代码，也就是我们在写的任何原型代码不需要太多的改变就能支持生产级别的各种特性（比如并行、steaming 等），具体来说会有这些优势：</p><ul><li>并行: 只要是整个 chain 中有可以并行的步骤就会自动的并行，来减少使用时的延迟。</li><li>自动的重试和 fallback: 大部分 chain 的组成部分都有自动的重试（比如因为网络原因的失败）和回退机制，来解决很多请求的出错问题。</li><li>对 chain 中间结果的访问，在旧的写法中很难访问中间的结果，而 LCEL 中可以方便的通过访问中间结果来进行调试和记录。</li><li>LCEL 会自动支持 LangSimith 进行可视化和记录。</li></ul><p>一条 Chain 组成的每个模块都是继承自 Runnable 这个接口，而一条 Chain 也是继承自这个接口，所以一条 Chain 也可以很自然的成为另一个 Chain 的一个模块。</p><p>任意的 Runnable 对象，都有几个常用的标准调用接口:</p><ul><li>invoke 基础调用</li><li>batch 批量调用</li><li>stream 流式返回结果</li><li>streamLog 流式返回结果，并返回中间的运行结果</li></ul><h1 id="参考文章" tabindex="-1">参考文章 <a class="header-anchor" href="#参考文章" aria-label="Permalink to &quot;参考文章&quot;">​</a></h1><ul><li><a href="https://juejin.cn/post/7359082665276440627" target="_blank" rel="noreferrer">https://juejin.cn/post/7359082665276440627</a></li><li><a href="https://js.langchain.com/v0.2/docs/how_to/" target="_blank" rel="noreferrer">https://js.langchain.com/v0.2/docs/how_to/</a></li><li><a href="https://platform.openai.com/docs/quickstart?context=node" target="_blank" rel="noreferrer">https://platform.openai.com/docs/quickstart?context=node</a></li><li><a href="https://js.langchain.com/v0.2/docs/integrations/text_embedding/baidu_qianfan/" target="_blank" rel="noreferrer">https://js.langchain.com/v0.2/docs/integrations/text_embedding/baidu_qianfan/</a></li><li><a href="https://js.langchain.com/v0.2/docs/integrations/chat/openai" target="_blank" rel="noreferrer">https://js.langchain.com/v0.2/docs/integrations/chat/openai</a></li></ul>',44),r=[c];function i(E,y,h,d,u,g){return a(),n("div",null,r)}const b=s(t,[["render",i]]);export{F as __pageData,b as default};
